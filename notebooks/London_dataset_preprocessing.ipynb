{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "389e81dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['USE_PYGEOS'] = '0'\n",
        "import sys\n",
        "module_path = os.path.abspath(os.path.join('..'))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "\n",
        "import json\n",
        "import glob\n",
        "import networkx as nx\n",
        "import geopandas as gpd\n",
        "import matplotlib as mpl\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from math import comb\n",
        "from datetime import date\n",
        "from tqdm.notebook import tqdm as tqdm_notebook\n",
        "from copy import deepcopy\n",
        "from itertools import islice,product\n",
        "from argparse import Namespace\n",
        "from functools import partial\n",
        "from pyproj import Proj\n",
        "from scipy.spatial import KDTree\n",
        "from networkx.classes.function import path_weight\n",
        "# from mpl_toolkits.basemap import Basemap as Basemap\n",
        "\n",
        "\n",
        "from shapely.geometry import Point,LineString, Polygon, mapping, MultiPoint, box\n",
        "\n",
        "\n",
        "from ticodm.utils import *\n",
        "from ticodm.notebook_functions import *\n",
        "from ticodm.spatial_interaction_model import ProductionConstrained\n",
        "from ticodm.spatial_interaction_model_mcmc import ProductionConstrainedMarkovChainMonteCarlo\n",
        "# from ticodm.contingency_table_mcmc import ContingencyTableMarkovChainMonteCarlo\n",
        "from ticodm.contingency_table import instantiate_ct\n",
        "from ticodm.sim_models.production_constrained import *\n",
        "\n",
        "# mpl.rcParams['agg.path.chunksize'] = 10000\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9975cbab",
      "metadata": {},
      "source": [
        "# London retail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b13e791f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Expertiment id\n",
        "dataset = 'london_retail'\n",
        "origin_sizes_df = pd.read_csv('../../NeuralABM/data/HarrisWilson/London_data/origin_sizes.csv',index_col=0)\n",
        "origin_sizes_arr = origin_sizes_df.values.ravel()\n",
        "\n",
        "dest_sizes_df = pd.read_csv('../../NeuralABM/data/HarrisWilson/London_data/dest_sizes.csv',index_col=0)\n",
        "dest_sizes_arr = dest_sizes_df.values.ravel()\n",
        "\n",
        "cost_matrix_df = pd.read_csv('../../NeuralABM/data/HarrisWilson/London_data/exp_distances.csv',index_col=0)\n",
        "cost_matrix_arr = -np.log(cost_matrix_df.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbce0804",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalise data\n",
        "origin_sizes_arr_normalised = origin_sizes_arr/np.sum(origin_sizes_arr)\n",
        "dest_sizes_arr_normalised = dest_sizes_arr/np.sum(dest_sizes_arr)\n",
        "cost_matrix_arr_normalised = cost_matrix_arr/np.sum(cost_matrix_arr.sum())\n",
        "\n",
        "print('Origin sizes sum',np.sum(origin_sizes_arr_normalised))\n",
        "print('Destination sizes sum',np.sum(dest_sizes_arr_normalised))\n",
        "print('Cost matrix sum',np.sum(cost_matrix_arr_normalised.sum()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94608ba5",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.hist(cost_matrix_arr.flatten(),bins=50)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f02eb7bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "np.savetxt('../data/inputs/london_retail/origin_sizes.txt',origin_sizes_arr)\n",
        "np.savetxt('../data/inputs/london_retail/dest_sizes.txt',dest_sizes_arr)\n",
        "np.savetxt('../data/inputs/london_retail/cost_mat.txt',cost_matrix_arr)\n",
        "np.savetxt('../data/inputs/london_retail/origin_sizes_normalised.txt',origin_sizes_arr_normalised)\n",
        "np.savetxt('../data/inputs/london_retail/dest_sizes_normalised.txt',dest_sizes_arr_normalised)\n",
        "np.savetxt('../data/inputs/london_retail/cost_mat_normalised.txt',cost_matrix_arr_normalised)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f746055",
      "metadata": {},
      "outputs": [],
      "source": [
        "origin_demand = np.loadtxt('../data/inputs/london_retail/origin_supply.txt')\n",
        "log_destination_attraction = np.loadtxt('../data/inputs/london_retail/log_destination_attraction.txt')\n",
        "destination_attraction = np.exp(log_destination_attraction)\n",
        "cost_matrix = np.loadtxt('../data/inputs/london_retail/normalised_cost_matrix.txt')\n",
        "P = np.loadtxt('../data/inputs/london_retail/P.txt')\n",
        "xd0 = np.loadtxt('../data/inputs/london_retail/xd0.txt')\n",
        "wd0 = np.exp(xd0)\n",
        "cost_mat = np.loadtxt('../data/inputs/london_retail/cost_mat.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4254ecf7",
      "metadata": {},
      "outputs": [],
      "source": [
        "for i,o in enumerate(P):\n",
        "    if not np.isclose(o,origin_sizes_arr_normalised[i]):\n",
        "        raise ValueError(f'NOT MATCHING {i} {o} vs {origin_sizes_arr_normalised[i]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23a0b8ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "for i,o in enumerate(wd0):\n",
        "    if not np.isclose(o,dest_sizes_arr_normalised[i]):\n",
        "        raise ValueError(f'NOT MATCHING {i} {o} vs {dest_sizes_arr_normalised[i]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d3acbc5",
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(np.shape(cost_mat)[0]):\n",
        "    for j in range(np.shape(cost_mat)[1]):\n",
        "        if not np.isclose(cost_mat[i,j],cost_matrix_arr_normalised[i,j]):\n",
        "            raise ValueError(f'NOT MATCHING {i},{j} {cost_mat[i,j]} vs {cost_matrix_arr_normalised[i,j]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d14b8e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "total_w = dest_sizes_arr_normalised.sum()\n",
        "min_w = np.min(dest_sizes_arr_normalised)\n",
        "total_o = origin_sizes_arr_normalised.sum()\n",
        "M = cost_matrix_arr_normalised.shape[1]\n",
        "# Compute kappa, delta\n",
        "kappa = total_o / (total_w - min_w*M)\n",
        "delta = kappa * min_w\n",
        "print(total_o,',',total_w)\n",
        "print(kappa,',',delta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d2dd6e9",
      "metadata": {},
      "outputs": [],
      "source": [
        "total_w = wd0.sum()\n",
        "min_w = np.min(wd0)\n",
        "total_o = P.sum()\n",
        "M = cost_mat.shape[1]\n",
        "# Compute kappa, delta\n",
        "kappa = total_o / (total_w - min_w*M)\n",
        "delta = kappa * min_w\n",
        "print(total_o,',',total_w)\n",
        "print(kappa,',',delta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd0a2f33",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Used kappa,delta\n",
        "used_delta = 0.3/M\n",
        "used_kappa = (total_o + used_delta*M)/total_w\n",
        "print(used_kappa,',',used_delta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29d16548",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_destination_attraction_by_cost.margins[tuplize(range(self.ndims()))]\n",
        "    dest_sizes_arr,\n",
        "    cms=[\n",
        "        cost_matrix\n",
        "    ],\n",
        "    cm_names=[\n",
        "        f'euclidean_distance'\n",
        "    ],\n",
        "    fig_size=(20,5)\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ticodm",
      "language": "python",
      "name": "ticodm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
