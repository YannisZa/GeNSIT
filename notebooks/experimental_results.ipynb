{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py as h5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "from multiresticodm.config import Config\n",
    "from multiresticodm.inputs import Inputs\n",
    "from multiresticodm.outputs import Outputs\n",
    "from multiresticodm.utils.misc_utils import *\n",
    "from multiresticodm.utils.math_utils import *\n",
    "from multiresticodm.utils.probability_utils import *\n",
    "from multiresticodm.contingency_table import instantiate_ct\n",
    "from multiresticodm.markov_basis import instantiate_markov_basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# AUTO RELOAD EXTERNAL MODULES\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_tables(out):\n",
    "    if out.inputs is None:\n",
    "        out.inputs = Inputs(\n",
    "            config = out.config,\n",
    "            synthetic_data = False,\n",
    "            logger = out.logger\n",
    "        )\n",
    "    else:\n",
    "        try:\n",
    "            out.inputs.cast_from_xarray()\n",
    "        except:\n",
    "            pass\n",
    "    ct = instantiate_ct(\n",
    "        config = out.config,\n",
    "        **out.inputs.data_vars(),\n",
    "        level = 'EMPTY'\n",
    "    )\n",
    "    samples = out.get_sample('table')\n",
    "    print(dict(samples.sizes))\n",
    "    print('axes constraints',ct.constraints['constrained_axes'])\n",
    "    print('cell constraints',len(ct.constraints['cells']))\n",
    "    tables_admissible = all([ct.table_admissible(torch.tensor(tab.values.squeeze())) for _,tab in samples.groupby('id')])\n",
    "    print('Tables admissible',tables_admissible)\n",
    "    if not tables_admissible:\n",
    "        print('Tables margins admissible',any([ct.table_margins_admissible(torch.tensor(tab.values.squeeze())) for _,tab in samples.groupby('id')]))\n",
    "        print('Tables cells admissible',all([ct.table_cells_admissible(torch.tensor(tab.values.squeeze())) for _,tab in samples.groupby('id')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get important paths\n",
    "experiment_id = 'NonJointTableSIM_NN_SweepedNoise_26_01_2024_13_26_18'\n",
    "# 'NonJointTableSIM_NN_SweepedNoise_26_01_2024_13_26_18'\n",
    "# 'NonJointTableSIM_NN_SweepedNoise_30_01_2024_23_25_12'\n",
    "# 'NonJointTableSIM_NN_SweepedNoise_01_02_2024_00_45_29'\n",
    "# 'JointTableSIM_NN_SweepedNoise_23_01_2024_21_33_25'\n",
    "# 'JointTableSIM_NN_SweepedNoise_30_01_2024_21_29_31'\n",
    "experiment_dir = f'../data/outputs/cambridge_work_commuter_lsoas_to_msoas/exp1/{experiment_id}/'\n",
    "relative_experiment_dir = os.path.relpath(experiment_dir,os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output processing settings\n",
    "settings = {\n",
    "    \"logging_mode\": \"INFO\",\n",
    "    \"coordinate_slice\": [\n",
    "        \"da.loss_name == str(['dest_attraction_ts_likelihood_loss'])\",\n",
    "        \"~da.title.isin([str('_unconstrained'), str('_total_constrained')])\"\n",
    "    ],\n",
    "    \"metadata_keys\":[],\n",
    "    \"burnin_thinning_trimming\": [{'iter': {\"burnin\":10000, \"thinning\":90, \"trimming\":1000}}],\n",
    "    \"n_workers\": 1,\n",
    "    \"group_by\":[\"seed\"],\n",
    "    \"filename_ending\":\"test\",\n",
    "    \"sample\":[\"table\"],\n",
    "    \"force_reload\":False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = 'loss_name_[dest_attraction_ts_likelihood_loss]/seed_23/'\n",
    "sigmas = ['low','high','learned']\n",
    "titles = ['_doubly_constrained','_doubly_10%_cell_constrained','_doubly_20%_cell_constrained']\n",
    "progress = tqdm(\n",
    "    total = len(sigmas)*len(titles),\n",
    "    desc = 'Loading sweep data'\n",
    ")\n",
    "for sig in sigmas:\n",
    "    for titl in titles:\n",
    "        # Create current sweep id\n",
    "        current_sweep_id = os.path.join('samples',sweep_id,f\"sigma_{sig}/title_{titl}/\")\n",
    "        \n",
    "        print(f\"sigma_{sig}/title_{titl}/\")\n",
    "        # Initialise outputs\n",
    "        current_sweep_outputs = Outputs(\n",
    "            config = os.path.join(relative_experiment_dir,current_sweep_id),\n",
    "            settings = settings,\n",
    "            inputs = None,\n",
    "            slice = True,\n",
    "            level = 'INFO'\n",
    "        )\n",
    "        # Silence outputs\n",
    "        current_sweep_outputs.logger.setLevels(console_level='EMPTY')\n",
    "        # Load all data\n",
    "        current_sweep_outputs.load()\n",
    "        # Get first collection id\n",
    "        current_sweep_outputs0 = current_sweep_outputs.get(0)\n",
    "        # Validate tables\n",
    "        # validate_tables(current_sweep_outputs0)\n",
    "        print('SRMSE',srmse(current_sweep_outputs0.data.table.mean('id'),current_sweep_outputs.inputs.data.ground_truth_table).values.squeeze())\n",
    "        print('\\n')\n",
    "        # break\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get important paths\n",
    "experiment_id = 'NonJointTableSIM_NN_SweepedNoise_01_02_2024_00_45_29'\n",
    "# 'NonJointTableSIM_NN_SweepedNoise_26_01_2024_13_26_18'\n",
    "# 'NonJointTableSIM_NN_SweepedNoise_30_01_2024_23_25_12'\n",
    "# 'NonJointTableSIM_NN_SweepedNoise_01_02_2024_00_45_29'\n",
    "# 'JointTableSIM_NN_SweepedNoise_23_01_2024_21_33_25'\n",
    "# 'JointTableSIM_NN_SweepedNoise_30_01_2024_21_29_31'\n",
    "experiment_dir = f'../data/outputs/cambridge_work_commuter_lsoas_to_msoas/exp1/{experiment_id}/'\n",
    "relative_experiment_dir = os.path.relpath(experiment_dir,os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise outputs\n",
    "current_sweep_outputs = Outputs(\n",
    "    config = relative_experiment_dir,\n",
    "    settings = settings,\n",
    "    inputs = None,\n",
    "    slice = True,\n",
    "    level = 'INFO'\n",
    ")\n",
    "# Silence outputs\n",
    "current_sweep_outputs.logger.setLevels(console_level='EMPTY')\n",
    "# Load all data\n",
    "current_sweep_outputs.load()\n",
    "# Get first collection id\n",
    "# current_sweep_outputs0 = current_sweep_outputs.get(0)\n",
    "# Validate tables\n",
    "# validate_tables(current_sweep_outputs0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporary scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = 'JointTableSIM_NN_SweepedNoise_23_01_2024_21_33_25'\n",
    "experiment_dir = f'../data/outputs/cambridge_work_commuter_lsoas_to_msoas/exp1/{experiment_id}/'\n",
    "relative_experiment_dir = os.path.relpath(experiment_dir,os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in os.walk(relative_experiment_dir):\n",
    "    if 'doubly' in d[0]:\n",
    "        for f in ['data.h5','metadata.json','outputs.log']:\n",
    "            if os.path.exists(os.path.join(d[0],f)):\n",
    "                os.remove(os.path.join(d[0],f))\n",
    "        if os.path.exists(d[0]):\n",
    "            os.rmdir(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sweep_config in tqdm(current_sweep_outputs.config.sweep_configurations,total=len(current_sweep_outputs.config.sweep_configurations)):\n",
    "    new_config,sweep = current_sweep_outputs.config.prepare_experiment_config(sweep_config)\n",
    "    sweep_id = current_sweep_outputs.config.get_sweep_id(sweep = sweep)\n",
    "    filepath = os.path.join(relative_experiment_dir,'samples',sweep_id,'metadata.json')\n",
    "    metadata = read_json(filepath = filepath)\n",
    "    metadata['neural_network']['loss'] = new_config['neural_network']['loss']\n",
    "    write_json(data = metadata, filepath = filepath)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrticodm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
