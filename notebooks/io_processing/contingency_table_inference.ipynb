{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b6f6df25",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import sys\n",
        "import json\n",
        "import gzip\n",
        "import glob\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn.manifold\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from math import comb\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "from argparse import Namespace\n",
        "from collections import OrderedDict\n",
        "from functools import partial\n",
        "from scipy.stats import multinomial\n",
        "\n",
        "from ticodm.utils import *\n",
        "from ticodm.spatial_interaction_model import ProductionConstrained\n",
        "from ticodm.contingency_table_mcmc import ContingencyTableMarkovChainMonteCarlo\n",
        "from ticodm.contingency_table import instantiate_ct\n",
        "\n",
        "mpl.rcParams['agg.path.chunksize'] = 10000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbf03972",
      "metadata": {},
      "source": [
        "# Import samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "236684d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Expertiment id\n",
        "experiment_data = 'synthetic_2x3_N_5000'\n",
        "experiment_id = 'exp3_direct_sampling'\n",
        "# Expertiment type\n",
        "experiment_type = 'TableMCMC'\n",
        "# Expertiment date\n",
        "date = '27_06_2022'\n",
        "# exp8c_TableMCMC_19_04_2022\n",
        "# Comment\n",
        "comment = 'using_direct_sampling' \n",
        "# comment = 'using_degree_higher_markov_basis'\n",
        "# comment = 'using_degree_one_markov_basis'\n",
        "# comment = 'using_direct_sampling'\n",
        "\n",
        "\n",
        "# Define directory\n",
        "dirpath = f'../data/outputs/{experiment_data}_{experiment_id}_{experiment_type}_{date}/'\n",
        "# Define filepaths\n",
        "metadata_filename = os.path.join(dirpath,f'{experiment_data}_{experiment_id}_{experiment_type}_{date}_metadata.json')\n",
        "table_filename = os.path.join(dirpath,f'samples/table*_samples.npy')\n",
        "table_filenames = glob.glob(table_filename)\n",
        "destination_dem_filename = os.path.join(dirpath,f'samples/destination_demand*_samples.npy')\n",
        "destination_dem_filenames = glob.glob(destination_dem_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e19669a8",
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../data/outputs/synthetic_2x3_N_5000_exp3_direct_sampling_TableMCMC_27_06_2022/synthetic_2x3_N_5000_exp3_direct_sampling_TableMCMC_27_06_2022_metadata.json'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/iz230@ad.eng.cam.ac.uk/ticodm/notebooks/Example 1 - Origin destination matrix inference.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcsic40/home/iz230%40ad.eng.cam.ac.uk/ticodm/notebooks/Example%201%20-%20Origin%20destination%20matrix%20inference.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Load files into memory\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcsic40/home/iz230%40ad.eng.cam.ac.uk/ticodm/notebooks/Example%201%20-%20Origin%20destination%20matrix%20inference.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(metadata_filename, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m fin:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcsic40/home/iz230%40ad.eng.cam.ac.uk/ticodm/notebooks/Example%201%20-%20Origin%20destination%20matrix%20inference.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     metadata \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(fin)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcsic40/home/iz230%40ad.eng.cam.ac.uk/ticodm/notebooks/Example%201%20-%20Origin%20destination%20matrix%20inference.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m N \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(metadata[\u001b[39m'\u001b[39m\u001b[39mmcmc\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mN\u001b[39m\u001b[39m'\u001b[39m])\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/outputs/synthetic_2x3_N_5000_exp3_direct_sampling_TableMCMC_27_06_2022/synthetic_2x3_N_5000_exp3_direct_sampling_TableMCMC_27_06_2022_metadata.json'"
          ]
        }
      ],
      "source": [
        "# Load files into memory\n",
        "with open(metadata_filename, 'r') as fin:\n",
        "    metadata = json.load(fin)\n",
        "N = int(metadata['mcmc']['N'])\n",
        "batch_size = int(metadata['store_progress']*N)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bba8c424",
      "metadata": {},
      "outputs": [],
      "source": [
        "print('MCMC took %s minutes and %s seconds' % (divmod(metadata['execution_time'], 60)))\n",
        "print(f\"{N} samples taken\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fffd217",
      "metadata": {},
      "outputs": [],
      "source": [
        "metadata['inputs']['dataset'] = '.'+metadata['inputs']['dataset']\n",
        "# Reconstruct expected flows\n",
        "metadata_copy = deepcopy(metadata)\n",
        "# del metadata_copy['mcmc']['contingency_table']['column_sum_proposal']\n",
        "metadata_copy['mcmc']['contingency_table']['proposal'] = 'direct_sampling'\n",
        "dummy_config = Namespace(**{'settings':metadata_copy})\n",
        "ct = instantiate_ct(dummy_config)\n",
        "sim = ProductionConstrained(dummy_config)\n",
        "ct_mcmc = ContingencyTableMarkovChainMonteCarlo(ct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "477124f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read important metadata (true latent values)\n",
        "true_table = ct.table\n",
        "colsums = true_table.sum(axis=0)\n",
        "rowsums = true_table.sum(axis=1)\n",
        "I,J = len(rowsums),len(colsums)\n",
        "log_lambdas = np.asarray(metadata['log_true_intensities'],dtype='float32')\n",
        "log_colsum_lambdas = np.ones(J)\n",
        "for j in range(J):\n",
        "    log_colsum_lambdas[j] = logsumexp(log_lambdas[:,j])\n",
        "# Get size of support over tables\n",
        "# THIS TAKES TOO LONG FOR LARGE TABLES - CANNOT ENUMERATE SUPPORT FAST\n",
        "# table_support_size = 1\n",
        "# for i in tqdm(range(I)):\n",
        "#     supp = [x for x in itertools.product(range(1,ct.rowsums[i]-J+2), repeat=J) if sum(x) == ct.rowsums[i]]\n",
        "#     assert not np.any([0 in x for x in supp])\n",
        "#     table_support_size *= len(supp)\n",
        "# I*np.prod([comb(ct.rowsums[i]+J-1,J-1) for i in range(I)])\n",
        "\n",
        "# Flag for re computing sample statistics\n",
        "recompute = True\n",
        "# Decide on figure format\n",
        "figure_format = 'eps' # 'eps','png'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cbc9af7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define sample sizes so that statistics will be compute every MCMC interval\n",
        "table_sample_step = 1\n",
        "assert N%table_sample_step == 0\n",
        "table_burnin = 0\n",
        "table_chain_length = int(20000)\n",
        "maxN = int(min(table_chain_length,N))\n",
        "table_sample_sizes = list(range(table_burnin,maxN,table_sample_step))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2f648b4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# f = gzip.GzipFile(\"../data/outputs/synthetic_2x3_exp8_degree_one_TableMCMC_31_05_2022/samples/table_samples.npy.gz\", \"r\")\n",
        "# samples = np.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf3f1032",
      "metadata": {},
      "outputs": [],
      "source": [
        "# write_npy(\n",
        "#     samples,\n",
        "#     \"../data/outputs/synthetic_2x3_exp8_degree_one_TableMCMC_31_05_2022/samples/destination_demand_samples.npy\"\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1194382",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialise running table mean and total samples\n",
        "running_table_mean = np.zeros((1,I,J),dtype='float32')\n",
        "total_samples = 0\n",
        "break_flag = False\n",
        "\n",
        "for i,file in tqdm(enumerate(sorted(table_filenames)),total=len(table_filenames)):\n",
        "    print(f'Reading batch {i}')\n",
        "    # Read table batch\n",
        "    table_sample_batch = np.load(file,mmap_mode='r')\n",
        "\n",
        "    # Compute running mean for current batch\n",
        "    print('Running average multivariate')\n",
        "    latest_running_means = running_average_multivariate(\n",
        "                                table_sample_batch,\n",
        "                                running_table_mean[-1],\n",
        "                                total_samples\n",
        "                            )\n",
        "    latest_batch_size = latest_running_means.shape[0]\n",
        "    last_sample = (table_chain_length-total_samples)\n",
        "    \n",
        "    # Clear memmory\n",
        "    del table_sample_batch\n",
        "    gc.collect()\n",
        "    print('Appending results and clearing memory')\n",
        "        \n",
        "    # Keep only Kth mean\n",
        "    if last_sample > latest_batch_size:\n",
        "        if i == 0:\n",
        "            running_table_mean = np.append(\n",
        "                                    latest_running_means[table_burnin::table_sample_step],\n",
        "                                    np.expand_dims(latest_running_means[-1],axis=0),\n",
        "                                    axis=0\n",
        "                                )\n",
        "        else:\n",
        "            # If the remaining samples to be averaged over exceed the batch size continue as normal\n",
        "            running_table_mean = np.concatenate(\n",
        "                                    [running_table_mean,\n",
        "                                    latest_running_means[table_sample_step::table_sample_step],\n",
        "                                    np.expand_dims(latest_running_means[-1],axis=0)],\n",
        "                                    axis=0\n",
        "                                )\n",
        "    else:\n",
        "        break_flag = True\n",
        "        if i == 0:\n",
        "            running_table_mean = latest_running_means[table_burnin:(last_sample):table_sample_step]\n",
        "        else:\n",
        "            # If the remaining samples to be averaged over exceed the batch size continue as normal\n",
        "            running_table_mean = np.concatenate(\n",
        "                                    [running_table_mean,\n",
        "                                    latest_running_means[table_sample_step:(last_sample):table_sample_step]],\n",
        "                                    axis=0\n",
        "                                )\n",
        "    \n",
        "    # Update total number of samples\n",
        "    total_samples += latest_running_means.shape[0]\n",
        "    latest_batch_size = latest_running_means.shape[0]\n",
        "    last_sample = (table_chain_length-total_samples)\n",
        "    \n",
        "    # Clear memory\n",
        "    del latest_running_means\n",
        "    gc.collect()\n",
        "    \n",
        "    # If total number of sample exceeds specified total - stop\n",
        "    if break_flag:\n",
        "        print('Breaking early. Found more samples than required.')\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f975b946",
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_mean_error_l1_norms = np.zeros(running_table_mean.shape[0])\n",
        "sample_mean_error_l2_norms = np.zeros(running_table_mean.shape[0])\n",
        "for i,s in tqdm(enumerate(table_sample_sizes),total=len(table_sample_sizes)):\n",
        "    sample_mean_error_l1_norms[i] = relative_l1(\n",
        "                        tab0=np.exp(log_lambdas),\n",
        "                        tab=running_table_mean[i]\n",
        "    )\n",
        "    sample_mean_error_l2_norms[i] = relative_l2_norm(\n",
        "                        tab0=np.exp(log_lambdas),\n",
        "                        tab=running_table_mean[i]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a0c8bf2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Define experiment directory\n",
        "# experiment_directory = os.path.basename(metadata['inputs']['dataset'])+'_'+metadata['experiment_id']+'_'+metadata['type']+'_'+metadata['datetime']\n",
        "# # Create subdirectory for estimator statistics\n",
        "# makedir(os.path.join(\n",
        "#         metadata['outputs']['directory'],\n",
        "#         experiment_directory,\n",
        "#         'sample_estimators')\n",
        "# )\n",
        "# print('Writing compressed npy. This is going to take a while...')\n",
        "# # Writing estimator data to compressed npy\n",
        "# write_compressed_npy(\n",
        "#     running_table_mean,\n",
        "#     os.path.join(\n",
        "#         metadata['outputs']['directory'],\n",
        "#         experiment_directory,\n",
        "#         'sample_estimators',\n",
        "#         f\"table_mean_burnin_{table_burnin}_N_{table_chain_length}_step_{table_sample_step}_{comment}.npy.gz\"\n",
        "#     )\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7699738",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(table_sample_sizes,sample_mean_error_l1_norms)\n",
        "plt.xlabel('MCMC iteration',fontsize=16)\n",
        "plt.ylabel(r'Relative $L_1$ of $\\mathbb{E}[\\mathbf{n}|\\mathbf{n}_{\\cdot,+}]$',fontsize=16)\n",
        "plt.locator_params(axis='x', nbins=10)\n",
        "plt.axhline(y=0,color='red')\n",
        "# plt.savefig(os.path.join(dirpath,f'figures/expected_table_relative_l1_with_mcmc_iteration_chain_length_{table_chain_length}_{comment}.{figure_format}'),format=figure_format)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13fe5401",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(table_sample_sizes,sample_mean_error_l2_norms)\n",
        "plt.xlabel('MCMC iteration',fontsize=16)\n",
        "plt.ylabel(r'Relative $L_2$ of $\\mathbb{E}[\\mathbf{n}|\\mathbf{n}_{\\cdot,+}]$',fontsize=16)\n",
        "plt.locator_params(axis='x', nbins=20)\n",
        "plt.axhline(y=0,color='red')\n",
        "# plt.savefig(os.path.join(dirpath,f'figures/expected_table_relative_l2_norm_with_mcmc_iteration_chain_length_{table_chain_length}_{comment}.{figure_format}'),format=figure_format)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3f33093",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define sample sizes so that statistics will be compute every MCMC interval\n",
        "colsum_sample_step = 1\n",
        "assert N%colsum_sample_step == 0\n",
        "colsum_burnin = 0\n",
        "colsum_chain_length = int(1e6)\n",
        "assert colsum_chain_length%batch_size == 0\n",
        "maxN = int(min(colsum_burnin+colsum_chain_length,N))\n",
        "colsum_sample_sizes = list(range(colsum_burnin,maxN+1,colsum_sample_step))\n",
        "colsum_sample_sizes[0] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "982fa964",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialise running table mean and total samples\n",
        "running_colsum_mean = np.zeros((1,J))\n",
        "total_colsum_samples = 0\n",
        "\n",
        "for i,file in tqdm(enumerate(sorted(destination_dem_filenames)),total=len(destination_dem_filenames)):\n",
        "    print(f'Reading file {i}')\n",
        "    # Read table batch\n",
        "    colsum_sample_batch = np.load(file,mmap_mode='r')\n",
        "\n",
        "    # Compute running mean for current batch\n",
        "    print('Running average multivariate')\n",
        "    latest_colsum_running_means = running_average_multivariate(\n",
        "                                        colsum_sample_batch,\n",
        "                                        running_colsum_mean,\n",
        "                                        total_colsum_samples\n",
        "                                )\n",
        "    # Clear memmory\n",
        "    del colsum_sample_batch\n",
        "    gc.collect()\n",
        "    \n",
        "    print('Appending results and clearing memmory')\n",
        "    # Keep only Kth mean\n",
        "    if i == 0:\n",
        "        running_colsum_mean = np.append(\n",
        "                                latest_colsum_running_means[0::colsum_sample_step],\n",
        "                                np.expand_dims(latest_colsum_running_means[-1],axis=0),\n",
        "                                axis=0\n",
        "                            )\n",
        "    else:\n",
        "        running_colsum_mean = np.concatenate(\n",
        "                                [running_colsum_mean,\n",
        "                                latest_colsum_running_means[table_sample_step::colsum_sample_step],\n",
        "                                np.expand_dims(latest_colsum_running_means[-1],axis=0)],\n",
        "                                axis=0\n",
        "                            )\n",
        "\n",
        "    # Update total number of samples\n",
        "    total_colsum_samples += latest_colsum_running_means.shape[0]\n",
        "    \n",
        "    # Clear memory\n",
        "    del latest_colsum_running_means\n",
        "    gc.collect()\n",
        "    \n",
        "    # If total number of sample exceeds specified total - stop\n",
        "    if total_colsum_samples >= (colsum_chain_length-batch_size):\n",
        "        print('Breaking early. Found more samples than required.')\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "592b67f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "colsum_sample_mean_error_l1_norms = np.zeros(running_colsum_mean.shape[0])\n",
        "colsum_sample_mean_error_l2_norms = np.zeros(running_colsum_mean.shape[0])\n",
        "ground_truth_colsum_intensities = np.exp(log_lambdas).sum(axis=0)\n",
        "for i,s in tqdm(enumerate(colsum_sample_sizes),total=colsum_sample_sizes.shape[0]):\n",
        "    colsum_sample_mean_error_l1_norms[i] = relative_l1(\n",
        "                        tab0=ground_truth_colsum_intensities,\n",
        "                        tab=running_colsum_mean[i]\n",
        "    )\n",
        "    colsum_sample_mean_error_l2_norms[i] = relative_l2_norm(\n",
        "                        tab0=ground_truth_colsum_intensities,\n",
        "                        tab=running_colsum_mean[i]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5729a9bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define experiment directory\n",
        "experiment_directory = os.path.basename(metadata['inputs']['dataset'])+'_'+metadata['experiment_id']+'_'+metadata['type']+'_'+metadata['datetime']\n",
        "# Create subdirectory for estimator statistics\n",
        "makedir(os.path.join(\n",
        "        metadata['outputs']['directory'],\n",
        "        experiment_directory,\n",
        "        'sample_estimators')\n",
        ")\n",
        "print('Writing compressed npy.')\n",
        "# Writing estimator data to compressed npy\n",
        "write_compressed_npy(\n",
        "    running_colsum_mean,\n",
        "    os.path.join(\n",
        "        metadata['outputs']['directory'],\n",
        "        experiment_directory,\n",
        "        'sample_estimators',\n",
        "        f\"colsum_mean_burnin_{colsum_burnin}_N_{colsum_chain_length}_step_{colsum_sample_step}_{comment}.npy.gz\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcddcef2",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(colsum_sample_sizes[0:stop],colsum_sample_mean_error_l1_norms[0:stop])\n",
        "plt.xlabel('MCMC iteration',fontsize=16)\n",
        "plt.ylabel(r'Relative $L_1$ of $\\mathbb{E}[\\mathbf{n}_{+,\\cdot}|\\mathbf{n}_{+,+}]$',fontsize=16)\n",
        "plt.locator_params(axis='x', nbins=10)\n",
        "plt.axhline(y=0,color='red')\n",
        "plt.savefig(os.path.join(dirpath,f'figures/expected_colsum_relative_l1_with_mcmc_iteration_N_{colsum_chain_length}_{comment}.{figure_format}'),format=figure_format)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cfc5ed9",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(colsum_sample_sizes[0:stop],colsum_sample_mean_error_l2_norms[0:stop])\n",
        "plt.xlabel('MCMC iteration',fontsize=16)\n",
        "plt.ylabel(r'Relative $L_2$ of $\\mathbb{E}[\\mathbf{n}_{+,\\cdot}|\\mathbf{n}_{+,+}]$',fontsize=16)\n",
        "plt.locator_params(axis='x', nbins=10)\n",
        "plt.axhline(y=0,color='red')\n",
        "plt.savefig(os.path.join(dirpath,f'figures/expected_colsum_relative_l2_norm_with_mcmc_iteration_N_{colsum_chain_length}_{comment}.{figure_format}'),format=figure_format)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db52ce25",
      "metadata": {},
      "source": [
        "# Postprocessing \n",
        "Computing statistics over samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb9e3115",
      "metadata": {},
      "outputs": [],
      "source": [
        "table_samples = []\n",
        "for file in sorted(glob.glob(table_filename)):\n",
        "    # Load files into memory\n",
        "    table_sample_batch = read_npy(file)\n",
        "    table_samples.append(table_sample_batch)\n",
        "table_samples = np.concatenate(table_samples,axis=0)\n",
        "N = table_samples.shape[0]\n",
        "\n",
        "colsum_samples = []\n",
        "for file in sorted(glob.glob(destination_dem_filename)):\n",
        "    # Load files into memory\n",
        "    colsum_sample_batch = read_npy(file)\n",
        "    colsum_samples.append(colsum_sample_batch)\n",
        "colsum_samples = np.concatenate(colsum_samples,axis=0)\n",
        "N = colsum_samples.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e713710e",
      "metadata": {},
      "outputs": [],
      "source": [
        "table_samples_flattened = table_samples.reshape(table_samples.shape[0],-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "884b3697",
      "metadata": {},
      "outputs": [],
      "source": [
        "# if recompute or not os.path.exists(os.path.join(dirpath,f'samples/table_strings.txt.gz')):\n",
        "table_strings = np.empty(table_samples.shape[0],dtype=object)\n",
        "for i,ts in tqdm(enumerate(table_samples),total=table_samples.shape[0]):\n",
        "    table_strings[i] = table_to_str(ts)\n",
        "#     np.savetxt(fname=os.path.join(dirpath,f'samples/table_strings.txt.gz'),X=np.asarray(table_strings), fmt=\"%s\")\n",
        "# else:\n",
        "#     table_strings = np.loadtxt(os.path.join(dirpath,f'samples/table_strings.txt.gz'),dtype=str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "640bc7c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# if recompute or not os.path.exists(os.path.join(dirpath,f'samples/column_sum_strings.txt.gz')):\n",
        "colsum_strings = np.empty(colsum_samples.shape[0],dtype=object)\n",
        "for i,cs in tqdm(enumerate(colsum_samples),total=colsum_samples.shape[0]):\n",
        "    colsum_strings[i] = table_to_str(cs.astype('int32'))\n",
        "# np.savetxt(fname=os.path.join(dirpath,f'samples/column_sum_strings.txt.gz'),X=np.asarray(colsum_strings), fmt=\"%s\")\n",
        "# else:\n",
        "#     colsum_strings = np.loadtxt(os.path.join(dirpath,f'samples/column_sum_strings.txt.gz'),dtype=str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5972b3ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "# if recompute or not os.path.exists(os.path.join(dirpath,f'samples/table_similarity.txt.gz')):\n",
        "# Compute table similarity (log target difference)\n",
        "tab0_evaluation = log_product_multinomial_pmf(true_table,log_lambdas)\n",
        "table_similarity_measure_partial = partial(\n",
        "                                        table_similarity_measure,\n",
        "                                        tab0=tab0_evaluation,\n",
        "                                        log_cell_intensities=np.ascontiguousarray(log_lambdas)\n",
        "                                    )\n",
        "table_similarity = np.empty(table_samples.shape[0])\n",
        "for i,ts in tqdm(enumerate(table_samples),total=table_samples.shape[0]):\n",
        "    table_similarity[i] = table_similarity_measure_partial(ts)\n",
        "# np.savetxt(fname=os.path.join(dirpath,f'samples/table_similarity.txt.gz'),X=np.asarray(table_similarity))\n",
        "# else:\n",
        "#     tab0_evaluation = log_product_multinomial_pmf(np.asarray(true_table),np.asarray(log_lambdas))\n",
        "#     table_similarity = np.loadtxt(os.path.join(dirpath,f'samples/table_similarity.txt.gz'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4738d4c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# if recompute or not os.path.exists(os.path.join(dirpath,f'samples/colsum_similarity.txt.gz')):\n",
        "# Compute table similarity (log target difference)\n",
        "colsum0_evaluation = log_multinomial_pmf(ct.colsums,np.log(ct.colsums))\n",
        "colsum_similarity_measure_partial = partial(\n",
        "                                            column_sum_similarity_measure,\n",
        "                                            csum0=colsum0_evaluation,\n",
        "                                            log_cell_intensities=np.ascontiguousarray(log_lambdas)\n",
        "                                    )\n",
        "colsum_similarity = np.empty(colsum_samples.shape[0])\n",
        "for i,cs in tqdm(enumerate(colsum_samples),total=colsum_samples.shape[0]):\n",
        "    colsum_similarity[i] = colsum_similarity_measure_partial(csum=cs.astype('int32'))\n",
        "#     sys.exit()\n",
        "# np.savetxt(fname=os.path.join(dirpath,f'samples/colsum_similarity.txt.gz'),X=np.asarray(colsum_similarity))\n",
        "# else:\n",
        "#     colsum0_evaluation = log_multinomial_pmf(ct.table,np.log(ct.true_colsums))\n",
        "#     colsum_similarity = np.loadtxt(os.path.join(dirpath,f'samples/colsum_similarity.txt.gz'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8623c88f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%time\n",
        "\n",
        "# if recompute or not os.path.exists(os.path.join(dirpath,f'samples/table_discrepancy.json.gz')):\n",
        "\n",
        "#     # Compute table differences\n",
        "#     with Pool(processes=3) as pool:\n",
        "#         table_difference_histogram_partial = partial(ct.table_difference_histogram,tab0=true_table)\n",
        "#         table_discrepancy = list(tqdm(pool.imap(table_difference_histogram_partial, table_samples), position=0, leave=True, total=len(table_samples)))\n",
        "\n",
        "#     with open(os.path.join(dirpath,f'samples/table_discrepancy.json.gz'), 'w') as outfile:\n",
        "#         outfile.write(json.dumps(table_discrepancy))\n",
        "# else:\n",
        "#     with open(os.path.join(dirpath,f'samples/table_discrepancy.json.gz')) as json_file:\n",
        "#         table_discrepancy = json.load(json_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3995fc4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collate all data into dataframe\n",
        "# if recompute or not os.path.exists(os.path.join(dirpath,f'samples/table_statistics.csv.gz')):\n",
        "table_data = []\n",
        "table_data_names = []\n",
        "var_names = ['table_similarity','colsum_similarity','table_discrepancy',\n",
        "             'table_distances','table_strings','colsum_strings']\n",
        "var_titles = ['table_similarity','colsum_similarity','discrepancy',\n",
        "              'distance','table_strings','colsum_strings']\n",
        "for attr,attr_name in zip(var_names,var_titles):\n",
        "    if attr in locals():\n",
        "        print(f'Adding {attr}')\n",
        "        table_data.append(np.asarray(globals()[attr]))\n",
        "        table_data_names.append(attr_name)\n",
        "table_data = np.asarray(table_data).reshape((len(table_data_names),len(table_samples)))\n",
        "table_stats = pd.DataFrame(table_data.T,index=range(len(table_samples)),columns=table_data_names)\n",
        "print('Built df')\n",
        "# if 'discrepancy' in table_stats.columns.values:\n",
        "#     table_stats[['+','-','0']] = pd.json_normalize(table_stats['discrepancy'])\n",
        "#     table_stats['not0'] = table_stats['+']+table_stats['-']\n",
        "#     table_stats.drop(['discrepancy'], axis=1,inplace=True)\n",
        "# if 'distance' in table_stats.columns.values:\n",
        "#     table_stats['updated_distance'] = table_stats['not0']*table_stats['distance']\n",
        "\n",
        "# Compute log target\n",
        "table_stats['table_similarity'] = table_stats['table_similarity'].astype('float32')\n",
        "table_stats['colsum_similarity'] = table_stats['colsum_similarity'].astype('float32')\n",
        "table_stats['table_log_target'] = -(table_stats['table_similarity'] - tab0_evaluation)\n",
        "table_stats['colsum_log_target'] = -(table_stats['colsum_similarity'] - colsum0_evaluation)\n",
        "\n",
        "# Export table to csv\n",
        "# table_stats.to_csv(os.path.join(dirpath,f'samples/table_statistics.csv.gz'))\n",
        "\n",
        "\n",
        "#     for attr,attr_name in zip(var_names,var_titles):\n",
        "#         if attr in locals():\n",
        "#             print(f'Removing {attr} from memmory') \n",
        "#             del globals()[attr]\n",
        "# else:\n",
        "#     table_stats = pd.read_csv(os.path.join(dirpath,f'samples/table_statistics.csv.gz'))\n",
        "    \n",
        "# if recompute or not os.path.exists(os.path.join(dirpath,f'samples/unique_table_statistics.csv.gz')):\n",
        "print('Getting unique table')\n",
        "# Create df with unique tables\n",
        "table_stats_unique = table_stats.drop_duplicates(subset=['table_strings'])\n",
        "column_stats_unique = table_stats.drop_duplicates(subset=['colsum_strings'])\n",
        "print('Adding table frequencies')\n",
        "table_stats_unique = pd.merge(table_stats_unique,table_stats.groupby('table_strings')['table_similarity'].count().reset_index(name='frequency'),on='table_strings',how='left')\n",
        "column_stats_unique = pd.merge(column_stats_unique,table_stats.groupby('colsum_strings')['colsum_similarity'].count().reset_index(name='frequency'),on='colsum_strings',how='left')\n",
        "# print('Storing dfs')\n",
        "# table_stats_unique.to_csv(os.path.join(dirpath,f'samples/unique_table_statistics.csv.gz'))\n",
        "# column_stats_unique.to_csv(os.path.join(dirpath,f'samples/unique_column_sum_statistics.csv.gz'))\n",
        "# else:\n",
        "#     table_stats_unique = pd.read_csv(os.path.join(dirpath,f'samples/unique_table_statistics.csv.gz'))\n",
        "#     table_stats_unique.drop(columns=['Unnamed: 0','Unnamed: 0.1'],inplace=True,errors='ignore')\n",
        "#     column_stats_unique = pd.read_csv(os.path.join(dirpath,f'samples/unique_column_sum_statistics.csv.gz'))\n",
        "#     column_stats_unique.drop(columns=['Unnamed: 0','Unnamed: 0.1'],inplace=True,errors='ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61c9b10d",
      "metadata": {},
      "source": [
        "# Table sampling convergence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b74a9580",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Get maximum a posteriori tables based on empirical distribution and log target distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "096e4e70",
      "metadata": {},
      "outputs": [],
      "source": [
        "maximum_a_posteriori_table = str_to_array(\n",
        "    table_stats_unique.sort_values('table_log_target',ascending=False).head(10)['table_strings'].values[0],dims=(I,J)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58cce5e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "maximum_a_posteriori_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4267847",
      "metadata": {},
      "outputs": [],
      "source": [
        "emprirical_maximum_a_posteriori_table = str_to_array(\n",
        "    table_stats_unique.sort_values('frequency',ascending=False).head(10)['table_strings'].values[1],dims=(I,J)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f00c5f41",
      "metadata": {},
      "outputs": [],
      "source": [
        "emprirical_maximum_a_posteriori_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0df04f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "true_table"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfbf8e50",
      "metadata": {},
      "source": [
        "## Compare empirical frequency of samples versus target measure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db276bf4",
      "metadata": {},
      "outputs": [],
      "source": [
        "table_stats_unique.sort_values('frequency',ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee4caca3",
      "metadata": {},
      "outputs": [],
      "source": [
        "str_to_table(table_stats_unique.sort_values('colsum_log_target',ascending=False).head(1)['colsum_strings'].values[0],dims=(1,J))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85a72543",
      "metadata": {},
      "outputs": [],
      "source": [
        "table_stats_unique.sort_values('table_log_target',ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba2f8f1a",
      "metadata": {},
      "source": [
        "## Table distance/similarity measures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ac25644",
      "metadata": {},
      "outputs": [],
      "source": [
        "# _ = table_stats.plot.scatter('distance','similarity', figsize=(10, 10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b423d677",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define metric of interest\n",
        "metric = 'table_similarity'\n",
        "# 'normed_inner_product'\n",
        "# 'table_similarity'\n",
        "# 'colsum_similarity'\n",
        "# 'distance'\n",
        "# 'updated_distance'\n",
        "# 'difference_frobenious_norm'\n",
        "metric_title = 'Log target difference from true table'\n",
        "# 'Normalised inner product wrt true table'\n",
        "# 'Normalised Manhattan distance from true table'\n",
        "# 'Log target difference from true column sums\n",
        "# 'Log target difference from true table'\n",
        "# 'Sum of differences from true table'\n",
        "# Resort table statistics based on metric of interest\n",
        "table_stats_unique = table_stats_unique.sort_values(metric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b58965e",
      "metadata": {},
      "outputs": [],
      "source": [
        "table_stats = table_stats.sort_index()\n",
        "table_stats.reset_index().iloc[table_burnin:,:].plot(y=metric,x='index',figsize=(20,10),legend=False)\n",
        "plt.xlabel('MCMC iteration',fontsize=20)\n",
        "plt.ylabel(metric_title,fontsize=20)\n",
        "# _ = plt.legend()\n",
        "plt.title(f'Burnin = {table_burnin}')\n",
        "# plt.savefig(os.path.join(dirpath,f'figures/{metric}_metric_versus_mcmc_iterations_{comment}.{figure_format}'),format=figure_format)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cac16e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# REALLY EXPENSIVE COMPUTATION\n",
        "# fig, (ax1,ax2) = plt.subplots(1,2, figsize=(10,10))\n",
        "# table_stats_unique[['0','+','-']].plot.barh(color={\"0\": \"green\", \"+\": \"red\", \"-\": \"orange\"},stacked=True,ax=ax1,width=1.0)\n",
        "# table_stats_unique[metric].plot.barh(color=\"blue\",stacked=True,ax=ax2,width=1.0)\n",
        "# ax1.set_title('Proportions of +ve,-ve and zero table differences from true table',fontsize=12)\n",
        "# _ = ax1.set_xticks([], [])\n",
        "# _ = ax1.set_yticks([], [])\n",
        "# _ = ax1.get_legend().remove()\n",
        "# _ = ax1.set_ylabel('Table samples',fontsize=16)\n",
        "# ax2.set_title(metric_title,fontsize=12)\n",
        "# _ = ax2.set_xticks([], [])\n",
        "# _ = ax2.set_yticks([], [])\n",
        "# fig.tight_layout()\n",
        "# fig.savefig(os.path.join(dirpath,f'figures/{metric}_metric_barplot_with_table_differences.eps'),format='eps')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19ea9141",
      "metadata": {},
      "source": [
        "## Table histograms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59dde7a7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count number of times true table was reconstructed\n",
        "# and build a histogram over table\n",
        "table_histogram = {}\n",
        "for tab in tqdm(table_samples[table_burnin::table_sample_step]):\n",
        "    # If encountered again add one to frequency\n",
        "    if table_to_str(tab) in table_histogram:\n",
        "        table_histogram[table_to_str(tab)] += 1\n",
        "    # If not encountered again create frequency 1\n",
        "    else:\n",
        "        table_histogram[table_to_str(tab)] = 1\n",
        "    \n",
        "# Sort histogram lexicographically\n",
        "# table_histogram = {k: v for k, v in sorted(table_histogram.items(), key=lambda item: item[0])}\n",
        "# Sort histogram by frequency\n",
        "table_histogram = {k: v for k, v in sorted(table_histogram.items(), key=lambda item: -item[1])}\n",
        "# Get total number of samples\n",
        "n_table_samples = table_samples.shape[0]\n",
        "# Normalise frequencies to get empirical probabilities\n",
        "table_probabilities = {k: v / n_table_samples for k, v in table_histogram.items()}\n",
        "\n",
        "if str_in_list(table_to_str(true_table),table_histogram.keys()):\n",
        "    print('True table frequency',table_histogram[table_to_str(true_table)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c773e446",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # THIS WILL NOT WORK BECAUSE I AM NOT OBSERVING THE ENTIRE SUPPORT OF TABLES!!!\n",
        "# ## Use table histogram to get support over all tables\n",
        "# true_log_target_distribution = {}\n",
        "# for str_tab in tqdm(table_histogram.keys()):\n",
        "#     true_log_target_distribution[str_tab] = log_product_multinomial_pmf(\n",
        "#             table=str_to_table(str_tab,dims=(I,J)),\n",
        "#             log_cell_intensities=log_lambdas\n",
        "#     ) + log_factorial(ct.margins[range(self.ndims())])\n",
        "# # Convert to pandas\n",
        "# # true_log_target_distribution_keys, true_log_target_distribution_values = true_target_distribution.keys(), true_target_distribution.values()\n",
        "# # true_log_target_distribution = pd.DataFrame({\n",
        "# #     \"strings\":true_log_target_distribution_keys,\n",
        "# #     \"frequency\":true_log_target_distribution_values\n",
        "# # })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "745af462",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check whether true table is identified and in which order\n",
        "# Check how many tables have the right column sums\n",
        "true_table_index = -1\n",
        "true_colsums_indices = []\n",
        "true_colsums_table_strings = []\n",
        "for i,keyvalue in tqdm(enumerate(table_histogram.items()),total=len(table_histogram.items())):\n",
        "    # Monitor correct table samples\n",
        "    if np.array_equal(str_to_table(keyvalue[0],dims=(I,J)),true_table):\n",
        "        true_table_index = i\n",
        "    # Monitor table samples with corrent column sums\n",
        "    if np.all(abs(str_to_table(keyvalue[0],dims=(I,J)).sum(axis=0) - true_table.sum(axis=0)) <= 1e-9):\n",
        "        true_colsums_indices.append(i)\n",
        "        true_colsums_table_strings.append(keyvalue[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "675e2bd4",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(table_histogram.keys()),f'distinct tables sampled.')\n",
        "# print(table_support_size,f'distinct tables exist in support.')\n",
        "print(f'True table was the {true_table_index+1} most frequently sampled table')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "443d7f20",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "plt.bar(range(len(table_histogram)), list(table_histogram.values()), align='center', label='samples')\n",
        "if len(true_colsums_indices) > 0:\n",
        "    plt.bar(true_colsums_indices, np.array(list(table_histogram.values()))[true_colsums_indices], align='center',color='green',label='true colsums')\n",
        "if true_table_index >= 0:\n",
        "    plt.bar(true_table_index, list(table_histogram.values())[true_table_index], align='center',color='red',label='true table')\n",
        "_ = plt.xlim(-1,200)\n",
        "_ = plt.ylabel('Frequency',fontsize=20)\n",
        "_ = plt.xlabel('Table string',fontsize=20)\n",
        "_ = plt.legend(fontsize=15)\n",
        "# plt.savefig(os.path.join(dirpath,f'figures/table_histogram_{comment}.{figure_format}'),format=figure_format)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d126011",
      "metadata": {},
      "outputs": [],
      "source": [
        "print('MAP table')\n",
        "str_to_table(list(table_histogram.keys())[0],dims=(I,J))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dae9511f",
      "metadata": {},
      "outputs": [],
      "source": [
        "print('True table')\n",
        "ct.table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42161f5d",
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Difference')\n",
        "str_to_table(list(table_histogram.keys())[0],dims=(I,J))-ct.table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d49c340b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Slice table histogram to obtain histogram of tables with true column sums\n",
        "true_table_histogram = OrderedDict((k, table_histogram[k]) for k in true_colsums_table_strings)\n",
        "# Sort histogram lexicographically\n",
        "true_table_histogram = {k: v for k, v in sorted(true_table_histogram.items(), key=lambda item: item[0])}\n",
        "# Store updated index of true table\n",
        "true_table_new_index = -1\n",
        "if str_in_list(table_to_str(true_table),true_table_histogram.keys()):\n",
        "    true_table_new_index = list(true_table_histogram.keys()).index(table_to_str(true_table))\n",
        "\n",
        "print(f\"{int(100*np.sum(list(true_table_histogram.values()))/len(table_samples[table_burnin::table_sample_step]))}% of table samples matched true column sums\")\n",
        "print('True table frequency',np.sum(list(true_table_histogram.values())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87b63125",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "if len(true_table_histogram) > 0:\n",
        "    plt.bar(range(len(true_table_histogram)), np.fromiter(true_table_histogram.values(),dtype=int), align='center',color='green',label='true colsums')\n",
        "if true_table_new_index > 0:\n",
        "    plt.bar(true_table_new_index, true_table_histogram[table_to_str(true_table)], align='center',color='red',label='true table')\n",
        "_ = plt.xticks(range(len(true_table_histogram)), list(true_table_histogram.keys()),rotation=50)\n",
        "_ = plt.xlim(-1,len(true_table_histogram))\n",
        "_ = plt.ylabel('Frequency',fontsize=20)\n",
        "_ = plt.xlabel('Table string',fontsize=20)\n",
        "_ = plt.legend(fontsize=20)\n",
        "# plt.savefig(os.path.join(dirpath,f'figures/true_table_histogram_{comment}.{figure_format}'),format=figure_format)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e57a7752",
      "metadata": {},
      "source": [
        "# margin sampling convergence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86337060",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Heuristically search for mode of column sum distribution (multinomial) based on generated probabilities\n",
        "colsum_distribution_mode,_,_,_,_ = ct_mcmc.mode_estimate_proposal_1way_table_multinomial(\n",
        "                                                    colsum_prev=colsums,\n",
        "                                                    log_cell_intensities=log_lambdas\n",
        "                                    )\n",
        "print('Column sums mode',colsum_distribution_mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea775b1c",
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Difference with true column sums',np.asarray(colsum_distribution_mode - colsums))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dbff534",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count number of times true table was reconstructed\n",
        "# and build a histogram over table\n",
        "colsum_histogram = {}\n",
        "for col in tqdm(colsum_samples):\n",
        "    if table_to_str(col.astype('int32')) in colsum_histogram:\n",
        "        colsum_histogram[table_to_str(col.astype('int32'))] += 1\n",
        "    else:\n",
        "        colsum_histogram[table_to_str(col.astype('int32'))] = 1\n",
        "    \n",
        "# Sort histogram lexicographically\n",
        "# colsum_histogram = {k: v for k, v in sorted(colsum_histogram.items(), key=lambda item: item[0])}\n",
        "# Sort histogram by frequency\n",
        "colsum_histogram = {k: v for k, v in sorted(colsum_histogram.items(), key=lambda item: -item[1])}\n",
        "\n",
        "n_colsum_samples = sum(colsum_histogram.values(), 0.0)\n",
        "colsum_probabilities = {k: v / n_colsum_samples for k, v in colsum_histogram.items()}\n",
        "total_flow = np.sum(colsums)\n",
        "\n",
        "if str_in_list(table_to_str(colsums),colsum_histogram.keys()):\n",
        "    print('True colsums frequency',colsum_histogram[table_to_str(colsums)])\n",
        "\n",
        "# Store frequency of samples according to multinomial\n",
        "_,log_probs,_ = ct_mcmc.log_intensities_to_multinomial_log_probabilities(log_lambdas)\n",
        "multinomial_colsums_histogram = {}\n",
        "for k in tqdm(colsum_histogram.keys()):\n",
        "    # Append table count to histogram\n",
        "    multinomial_colsums_histogram[k] = round(n_colsum_samples*multinomial.pmf(str_to_array(k,dims=(J)),n=total_flow,p=np.exp(log_probs)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f56fab3b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check whether true table is identified\n",
        "# Check how many tables have the right column sums\n",
        "true_colsums_index = -1\n",
        "for i,keyvalue in enumerate(colsum_histogram.items()):\n",
        "    # Monitor table samples with corrent column sums\n",
        "    if np.all(abs(str_to_array(keyvalue[0],dims=(J)) - colsums) <= 1e-9):\n",
        "        true_colsums_index = i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a069f11",
      "metadata": {},
      "outputs": [],
      "source": [
        "width = 1\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.bar(range(len(colsum_histogram)), list(colsum_histogram.values()), align='center',label='samples', width = width)\n",
        "# plt.bar(np.array(range(len(multinomial_colsums_histogram)))-width, list(multinomial_colsums_histogram.values()), align='center',label='multinomial', width = width)\n",
        "# _ = plt.xticks(range(len(colsum_histogram)), list(colsum_histogram.keys()),rotation=80)\n",
        "plt.ylabel('Frequency',fontsize=20)\n",
        "plt.xlabel('Table string',fontsize=20)\n",
        "plt.xlim(-1,len(colsum_histogram))\n",
        "plt.bar(true_colsums_index, list(colsum_histogram.values())[true_colsums_index], fill=False, linewidth=width, align='center',edgecolor='red',label='true', width = width)\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(dirpath,f'figures/true_colsums_histogram_{comment}.{figure_format}'),format=figure_format)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a1d959f",
      "metadata": {},
      "source": [
        "# Table and margin convergence\n",
        "## Define metrics\n",
        "1. Total variation distance between empirical measure and target measure\n",
        "1. Kullback Leibler divergence between empirical measure and target measure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b00b65a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute total probability of observed support\n",
        "total_probability = 0\n",
        "min_probability,max_probability = 1,0\n",
        "for k in np.unique(table_strings):\n",
        "    prob = np.exp(log_product_multinomial_pmf(str_to_table(k,dims=(I,J)),log_lambdas) + np.sum(log_factorial_vectorised(1,ct.rowsums)))\n",
        "    if prob < min_probability:\n",
        "        min_probability = prob\n",
        "    elif prob > max_probability:\n",
        "        max_probability = prob\n",
        "    total_probability += prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa9570df",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define total variation\n",
        "def total_variation(table_statistics,support,log_cell_intensities,total_prob,column_name:str='table_strings'):\n",
        "    # Get emprical distribution of tables from samples\n",
        "    empirical_log_distribution = table_statistics.groupby(\n",
        "                                [column_name]\n",
        "                            ).agg(\n",
        "                                {column_name:'count'}\n",
        "                            ).rename(\n",
        "                                columns={column_name:\"frequency\"}\n",
        "                            ).reset_index().values\n",
        "    # Convert two lists into dict\n",
        "    log_distribution = np.log(empirical_log_distribution[:,1].astype('float32')) - np.log(np.sum(empirical_log_distribution[:,1]))\n",
        "    empirical_log_distribution = dict(zip(empirical_log_distribution[:,0],log_distribution))\n",
        "    # Compute total variation\n",
        "    tv = 0\n",
        "    max_prob = 0\n",
        "    for k in support:\n",
        "        true_probability = np.exp(log_product_multinomial_pmf(str_to_table(k,dims=(I,J)),log_cell_intensities) + np.sum(log_factorial_vectorised(1,ct.rowsums))) / total_prob\n",
        "        if true_probability > max_prob:\n",
        "            max_prob = true_probability\n",
        "        if k in empirical_log_distribution.keys():\n",
        "            if abs(true_probability - np.exp(empirical_log_distribution[k])) > tv:\n",
        "                tv = abs(true_probability - np.exp(empirical_log_distribution[k]))\n",
        "        else:\n",
        "            if true_probability > tv:\n",
        "                tv = true_probability\n",
        "    return tv,max_prob\n",
        "\n",
        "# Define kl divergence\n",
        "def kl_divergence(table_statistics,support,log_cell_intensities,total_prob,overflow:float=1e-50,column_name:str='table_strings'):\n",
        "    # Get emprical distribution of tables from samples\n",
        "    empirical_log_distribution = table_statistics.groupby(\n",
        "                                [column_name]\n",
        "                            ).agg(\n",
        "                                {column_name:'count'}\n",
        "                            ).rename(\n",
        "                                columns={column_name:\"frequency\"}\n",
        "                            ).reset_index().values\n",
        "    # Convert two lists into dict\n",
        "    log_distribution = np.log(empirical_log_distribution[:,1].astype('float32')) - np.log(np.sum(empirical_log_distribution[:,1]))\n",
        "    empirical_log_distribution = dict(zip(empirical_log_distribution[:,0],log_distribution))\n",
        "    # Compute kullback leibler divergence\n",
        "    kl = 0\n",
        "    max_prob = 0\n",
        "    for k in support:\n",
        "        true_probability = np.exp(log_product_multinomial_pmf(str_to_table(k,dims=(I,J)),log_cell_intensities) + np.sum(log_factorial_vectorised(1,ct.rowsums))) / total_prob\n",
        "        if true_probability > max_prob:\n",
        "            max_prob = true_probability\n",
        "        if k in empirical_log_distribution.keys():\n",
        "            kl += true_probability*(np.log(true_probability) - empirical_log_distribution[k])\n",
        "        else:\n",
        "            kl += true_probability*(np.log(true_probability) - np.log(overflow))\n",
        "    return kl,max_prob\n",
        "\n",
        "# Define posterior probability mass coverage %\n",
        "def mass_coverage(table_statistics,log_cell_intensities,column_name:str='table_strings'):\n",
        "    # Get emprical distribution of tables from samples\n",
        "    empirical_log_distribution = table_statistics.groupby(\n",
        "                                [column_name]\n",
        "                            ).agg(\n",
        "                                {column_name:'count'}\n",
        "                            ).rename(\n",
        "                                columns={column_name:\"frequency\"}\n",
        "                            ).reset_index().values\n",
        "    # Convert two lists into dict\n",
        "    log_distribution = np.log(empirical_log_distribution[:,1].astype('float32')) - np.log(np.sum(empirical_log_distribution[:,1]))\n",
        "    empirical_log_distribution = dict(zip(empirical_log_distribution[:,0],log_distribution))\n",
        "    # Compute total probability of explored support under true distribution\n",
        "    total_prob = 0\n",
        "    for k in empirical_log_distribution.keys():\n",
        "        total_prob += np.exp(log_product_multinomial_pmf(str_to_table(k,dims=(I,J)),log_cell_intensities) + np.sum(log_factorial_vectorised(1,ct.rowsums)))\n",
        "    return total_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "589eef35",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define sample sizes so that statistics will be compute every MCMC interval\n",
        "table_sample_step = 1000\n",
        "table_burnin = 0\n",
        "table_chain_length = 1e7\n",
        "maxN = int(min(table_burnin+table_chain_length,table_samples.shape[0]))\n",
        "table_sample_sizes = range(table_burnin+table_sample_step,maxN+table_sample_step,table_sample_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e501b80b",
      "metadata": {},
      "outputs": [],
      "source": [
        "table_tvs = np.ones(len(table_sample_sizes)) * (-1)\n",
        "table_kls = np.ones(len(table_sample_sizes)) * (-1)\n",
        "table_mass_coverage = np.ones(len(table_sample_sizes)) * (-1)\n",
        "for i,s in tqdm(enumerate(table_sample_sizes),total=len(table_sample_sizes)):\n",
        "#     table_tvs[i],max_probability = total_variation(\n",
        "#                                         table_statistics=table_stats.iloc[table_burnin:s,:],\n",
        "#                                         log_cell_intensities=log_lambdas,\n",
        "#                                         support = np.unique(table_strings),\n",
        "#                                         total_prob = total_probability,\n",
        "#                                         column_name='table_strings'\n",
        "#                                     )\n",
        "#     table_kls[i],max_probability = kl_divergence(\n",
        "#                                         table_statistics=table_stats.iloc[table_burnin:s,:],\n",
        "#                                         log_cell_intensities=log_lambdas,\n",
        "#                                         support = np.unique(table_strings),\n",
        "#                                         total_prob = total_probability,\n",
        "#                                         overflow = 0.1*min_probability,\n",
        "#                                         column_name='table_strings'\n",
        "#                                     )\n",
        "    table_mass_coverage[i] = mass_coverage(\n",
        "                                table_stats.iloc[table_burnin:s,:],\n",
        "                                log_lambdas,\n",
        "                                column_name='table_strings'\n",
        "                            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b87211b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write to file\n",
        "write_npy(\n",
        "    np.array([table_sample_sizes,table_tvs]),\n",
        "    os.path.join(dirpath,f'samples/table_empirical_distribution_total_variation_with_sample_size_max_chain_length_{table_chain_length}_{comment}.gz.npy')\n",
        ")\n",
        "# Write to file\n",
        "write_npy(\n",
        "    np.array([table_sample_sizes,table_kls]),\n",
        "    os.path.join(dirpath,f'samples/table_empirical_distribution_kl_divergence_with_sample_size_max_chain_length_{table_chain_length}_{comment}.gz.npy')\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39020505",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(table_sample_sizes,table_mass_coverage)\n",
        "plt.title('Table empirical distribution convergence rate',fontsize=18)\n",
        "plt.xlabel('Number of samples',fontsize=16)\n",
        "plt.ylabel('% of probability mass explored',fontsize=16)\n",
        "plt.locator_params(axis='x', nbins=20)\n",
        "plt.savefig(os.path.join(dirpath,f'figures/table_total_probability_coverage_with_sample_size_max_chain_length_{table_chain_length}_{comment}.{figure_format}'),format=figure_format)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf5f0f57",
      "metadata": {},
      "outputs": [],
      "source": [
        "# table_mass_coverage_direct_sampling = deepcopy(table_mass_coverage)\n",
        "# table_mass_coverage_degree_higher = deepcopy(table_mass_coverage)\n",
        "# table_mass_coverage_degree_one = deepcopy(table_mass_coverage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60754933",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(table_sample_sizes,table_mass_coverage_direct_sampling,label='direct sampling')\n",
        "plt.plot(table_sample_sizes,table_mass_coverage_degree_higher,label='MBMCMC step size > 1')\n",
        "plt.plot(table_sample_sizes,table_mass_coverage_degree_one,label='MBMCMC step size = 1')\n",
        "# plt.title('Table empirical distribution convergence rate',fontsize=18)\n",
        "plt.xlabel('Number of samples',fontsize=16)\n",
        "plt.ylabel('% of probability mass explored',fontsize=16)\n",
        "plt.locator_params(axis='x', nbins=20)\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(dirpath,f'figures/table_total_probability_coverage_with_sample_size_max_chain_length_{table_chain_length}_comparison.{figure_format}'),format=figure_format)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a50f966c",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(table_sample_sizes,table_tvs)\n",
        "plt.title('Table empirical distribution convergence rate',fontsize=18)\n",
        "plt.axhline(y=0,color='red')\n",
        "plt.axhline(y=max_probability,color='purple')\n",
        "plt.xlabel('Number of samples',fontsize=16)\n",
        "plt.ylabel('Total variation',fontsize=16)\n",
        "plt.locator_params(axis='x', nbins=20)\n",
        "plt.savefig(os.path.join(dirpath,f'figures/table_empirical_distribution_total_variation_with_sample_size_max_chain_length_{table_chain_length}_{comment}.{figure_format}'),format=figure_format)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7246899a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# THIS IS PROBLEMATIC\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(table_sample_sizes,table_kls)\n",
        "plt.title('Table empirical distribution convergence rate',fontsize=18)\n",
        "plt.axhline(y=0,color='red')\n",
        "# plt.axhline(y=max_probability,color='purple')\n",
        "plt.xlabel('Number of samples',fontsize=16)\n",
        "plt.ylabel('KL divergence',fontsize=16)\n",
        "plt.locator_params(axis='x', nbins=20)\n",
        "plt.savefig(os.path.join(dirpath,f'figures/table_empirical_distribution_kl_divergence_with_sample_size_max_chain_length_{table_chain_length}_{comment}.{figure_format}'),format=figure_format)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e63f8ce",
      "metadata": {},
      "source": [
        "### Check for convergence in probability of table mean estimator\n",
        "This checks if weak law of large numbers holds by ensuring that\n",
        "\n",
        "$$\\lim_{M\\to \\infty} |\\mathbf{\\bar{n}}^{(0:M)} - \\boldsymbol{\\lambda}|^p = 0$$\n",
        "\n",
        "for $p=1,2$. The norm is defined as follows:\n",
        "\n",
        "$$|\\mathbf{n}|^p = \\left( \\sum_{i,j}^{I,J} |n_{ij}|^p \\right)^{1/p}.$$\n",
        "\n",
        "The goal is to establish that \n",
        "$$\\mathbf{\\bar{n}}^{(0:i)} = \\frac{1}{M}\\sum_{m=1}^M \\mathbf{n}^{(m)} \\to \\left( \\frac{O_i \\lambda_{ij}}{\\sum_{l}^J \\lambda_{il}} \\right)_{i,j}^{I,J} = \\boldsymbol{\\lambda},$$\n",
        "i.e. that the estimator $\\mathbf{\\bar{n}}^{(0:i)}$ converges in probability to the ground truth mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00e69986",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define sample sizes so that statistics will be compute every MCMC interval\n",
        "table_sample_step = 1000\n",
        "table_burnin = 0\n",
        "table_chain_length = N\n",
        "maxN = int(min(table_burnin+table_chain_length,table_samples.shape[0]))\n",
        "table_sample_sizes = range(table_burnin+table_sample_step,maxN+table_sample_step,table_sample_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f506eea9",
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_mean_error_l1_norms = np.zeros(len(table_sample_sizes))\n",
        "sample_mean_error_l2_norms = np.zeros(len(table_sample_sizes))\n",
        "for i,s in tqdm(enumerate(table_sample_sizes),total=len(table_sample_sizes)):\n",
        "    sample_mean_error_l1_norms[i] = relative_l1(\n",
        "                        tab0=np.exp(log_lambdas),\n",
        "                        tab=latest_mean\n",
        "    )\n",
        "    sample_mean_error_l2_norms[i] = relative_l2_norm(\n",
        "                        tab0=np.exp(log_lambdas),\n",
        "                        tab=latest_mean\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc9b0171",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(table_sample_sizes,sample_mean_error_l1_norms)\n",
        "plt.xlabel('MCMC iteration',fontsize=16)\n",
        "plt.ylabel(r'Relative $L_1$ of $\\mathbb{E}[\\mathbf{n}|\\mathbf{n}_{\\cdot,+}]$',fontsize=16)\n",
        "# plt.xticks(range(burnin,max(sample_sizes),(max(sample_sizes)-burnin)//10))\n",
        "plt.locator_params(axis='x', nbins=20)\n",
        "plt.axhline(y=0,color='red')\n",
        "plt.savefig(os.path.join(dirpath,f'figures/expected_table_relative_l1_with_mcmc_iteration_chain_length_{table_chain_length}_{comment}.{figure_format}'),format=figure_format)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bebf5b40",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(table_sample_sizes,sample_mean_error_l2_norms)\n",
        "plt.xlabel('MCMC iteration',fontsize=16)\n",
        "plt.ylabel(r'Relative $L_2$ of $\\mathbb{E}[\\mathbf{n}|\\mathbf{n}_{\\cdot,+}]$',fontsize=16)\n",
        "# plt.xticks(sample_sizes[::50])\n",
        "plt.locator_params(axis='x', nbins=20)\n",
        "plt.axhline(y=0,color='red')\n",
        "plt.savefig(os.path.join(dirpath,f'figures/expected_table_relative_l2_norm_with_mcmc_iteration_chain_length_{table_chain_length}_{comment}.{figure_format}'),format=figure_format)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb99d870",
      "metadata": {},
      "source": [
        "### Check for convergence in probability of margin mean estimator\n",
        "This checks if weak law of large numbers holds by ensuring that\n",
        "\n",
        "$$\\lim_{M\\to \\infty} |\\mathbf{\\bar{n}}_{\\cdot,+}^{(0:M)} - \\boldsymbol{\\sum_{i=1}^I \\lambda_{i,\\cdot}}|^p = 0$$\n",
        "\n",
        "for $p=1,2$. The norm is defined as follows:\n",
        "\n",
        "$$|\\mathbf{n}|^p = \\left( \\sum_{i,j}^{I,J} |n_{ij}|^p \\right)^{1/p}.$$\n",
        "\n",
        "The goal is to establish that \n",
        "$$\\mathbf{\\bar{n}}_{\\cdot,+}^{(0:i)} = \\frac{1}{M}\\sum_{m=1}^M \\mathbf{n}_{\\cdot,+}^{(m)} \\to \\left( \\frac{N \\sum_{i=1}^I\\lambda_{ij}}{\\sum_{i',j'}^{I,J} \\lambda_{i'j'}} \\right)_{i,j}^{I,J} = \\sum_{i=1}^I \\lambda_{i,j} \\; \\forall j,$$\n",
        "i.e. that the estimator $\\mathbf{\\bar{n}}_{\\cdot,+}^{(0:i)}$ converges in probability to the ground truth mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cceef41",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define sample sizes so that statistics will be compute every MCMC interval\n",
        "table_sample_step = 1\n",
        "table_burnin = 0\n",
        "table_chain_length = 2000\n",
        "maxN = int(min(table_burnin+table_chain_length,colsum_samples.shape[0]))\n",
        "table_sample_sizes = range(table_burnin+1,maxN+table_sample_step,table_sample_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86033933",
      "metadata": {},
      "outputs": [],
      "source": [
        "colsum_sample_mean_error_l1_norms = np.zeros(len(table_sample_sizes))\n",
        "colsum_sample_mean_error_l2_norms = np.zeros(len(table_sample_sizes))\n",
        "ground_truth_colsum_intensities = np.exp(log_lambdas).sum(axis=0)\n",
        "for i,s in tqdm(enumerate(table_sample_sizes),total=len(table_sample_sizes)):\n",
        "    colsum_sample_mean_error_l1_norms[i] = relative_l1(\n",
        "                        tab0=ground_truth_colsum_intensities,\n",
        "                        tab=np.mean(colsum_samples[table_burnin:s],axis=0)\n",
        "    )\n",
        "    colsum_sample_mean_error_l2_norms[i] = relative_l2_norm(\n",
        "                        tab0=ground_truth_colsum_intensities,\n",
        "                        tab=np.mean(colsum_samples[table_burnin:s],axis=0)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5b99dc2",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(table_sample_sizes,colsum_sample_mean_error_l1_norms)\n",
        "plt.xlabel('MCMC iteration',fontsize=16)\n",
        "plt.ylabel(r'Relative $L_1$ of $\\mathbb{E}[\\mathbf{n}_{+,\\cdot}|\\mathbf{n}_{+,+}]$',fontsize=16)\n",
        "# plt.xticks(sample_sizes[::50])\n",
        "plt.locator_params(axis='x', nbins=20)\n",
        "plt.axhline(y=0,color='red')\n",
        "plt.savefig(os.path.join(dirpath,f'figures/expected_colsum_relative_l1_with_mcmc_iteration_chain_length_{table_chain_length}_{comment}.{figure_format}'),format=figure_format)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dacd00ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(table_sample_sizes,colsum_sample_mean_error_l2_norms)\n",
        "plt.xlabel('MCMC iteration',fontsize=16)\n",
        "plt.ylabel(r'Relative $L_2$ of $\\mathbb{E}[\\mathbf{n}_{+,\\cdot}|\\mathbf{n}_{+,+}]$',fontsize=16)\n",
        "# plt.xticks(sample_sizes[::50])\n",
        "plt.locator_params(axis='x', nbins=20)\n",
        "plt.axhline(y=0,color='red')\n",
        "plt.savefig(os.path.join(dirpath,f'figures/expected_colsum_relative_l2_norm_with_mcmc_iteration_chain_length_{table_chain_length}_{comment}.{figure_format}'),format=figure_format)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd69f9ac",
      "metadata": {},
      "source": [
        "### Check for convergence in probability of table variance estimator [problematic]\n",
        "NOTE: THERE IS SOMETHING WRONG IN THE DEFINITION OF THE VARIANCE OF THE TABLE\n",
        "Let the empirical standard deviation be\n",
        "\n",
        "$$\\hat{\\sigma}^{(M)}_{\\mathbf{n}|\\mathbf{n_{\\cdot,+}}} = \\sqrt{\\frac{1}{M-1}\\sum_{m=1}^{M} (\\mathbf{n}^{(m)} - \\mathbf{\\bar{n}})^2},$$\n",
        "where $\\mathbf{\\bar{n}} = \\frac{1}{M} \\sum_{m=1}^M \\mathbf{n}^{(m)}$, $\\mathbf{n}^{(m)} \\sim p(\\mathbf{n}|\\mathbf{n_{\\cdot,+}})$.\n",
        "\n",
        "Under the ground truth intensities, the true variance is $\\mathbb{V}\\left[\\mathbf{n}|\\mathbf{n_{\\cdot,+}} \\right] = \\left( \\frac{O_i \\lambda_{ij}}{\\sum_{q=1}^J \\lambda_{iq}} \\right)_{i,j=1}^{I,J} = \\boldsymbol{\\lambda}$.\n",
        "\n",
        "The following checks if the weak law of large numbers holds by ensuring that\n",
        "\n",
        "$$\\lim_{M\\to \\infty} |(\\hat{\\sigma}^{(M)}_{\\mathbf{n}|\\mathbf{n_{\\cdot,+}}})^{2} - \\boldsymbol{\\lambda}|^r = 0$$\n",
        "\n",
        "for $r=1,2$. The norm is defined as follows:\n",
        "\n",
        "$$|\\mathbf{n}|^r = \\left( \\sum_{i,j}^{I,J} |n_{ij}|^r \\right)^{1/r}.$$\n",
        "\n",
        "The goal is to establish that \n",
        "$$p(|\\hat{\\sigma}^{(M)}_{\\mathbf{n}|\\mathbf{n_{\\cdot,+}}} - \\boldsymbol{\\lambda}|^r \\leq \\epsilon) = 1$$\n",
        "for arbitrary $\\epsilon > 0$.\n",
        "i.e. that the estimator $\\mathbf{\\bar{n}}^{(0:i)}$ converges in probability to the ground truth variance.\n",
        "\n",
        "The problem in the above is that we cannot get an unbiased estimator of $\\mathbb{V}\\left[\\mathbf{n}|\\mathbf{n}_{\\cdot,+}\\right]$ easily."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "505a473d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define sample sizes so that statistics will be compute every MCMC interval\n",
        "table_sample_step = 1\n",
        "table_burnin = 0\n",
        "table_chain_length = 500#00\n",
        "maxN = int(min(table_burnin+table_chain_length,table_samples.shape[0]))\n",
        "table_sample_sizes = range(table_burnin+table_sample_step,maxN+table_sample_step,table_sample_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08ab5661",
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_variance_error_l1_norms = np.zeros(len(table_sample_sizes))\n",
        "sample_variance_error_l2_norms = np.zeros(len(table_sample_sizes))\n",
        "for i,s in tqdm(enumerate(table_sample_sizes),total=len(table_sample_sizes)):\n",
        "    sample_variance_error_l1_norms[i] = relative_l1(\n",
        "                        tab0=np.sqrt(np.exp(log_lambdas)),\n",
        "                        tab=np.std(table_samples[table_burnin:s],axis=0,ddof=1)\n",
        "    )\n",
        "    sample_variance_error_l2_norms[i] = relative_l2_norm(\n",
        "                        tab0=np.sqrt(np.exp(log_lambdas)),\n",
        "                        tab=np.std(table_samples[table_burnin:s],axis=0,ddof=1)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0731071",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(table_sample_sizes,sample_variance_error_l2_norms)\n",
        "plt.xlabel('MCMC iteration',fontsize=16)\n",
        "plt.ylabel(r'Relative $L_2$ error of sample variance',fontsize=16)\n",
        "plt.locator_params(axis='x', nbins=20)\n",
        "plt.axhline(y=0,color='red')\n",
        "plt.savefig(os.path.join(dirpath,f'figures/relative_l2_error_sample_var_with_mcmc_iteration_{comment}.{figure_format}'),format=figure_format)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c3bfa31",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(table_sample_sizes,sample_variance_error_l1_norms)\n",
        "plt.xlabel('MCMC iteration',fontsize=16)\n",
        "plt.ylabel(r'Relative $L_1$ error of sample variance',fontsize=16)\n",
        "plt.locator_params(axis='x', nbins=20)\n",
        "plt.axhline(y=0,color='red')\n",
        "plt.savefig(os.path.join(dirpath,f'figures/relative_l1_error_sample_var_with_mcmc_iteration_{comment}.{figure_format}'),format=figure_format)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b731b7bb",
      "metadata": {},
      "source": [
        "## Convergence of second largest eigenvalue of transition matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fc30890",
      "metadata": {},
      "outputs": [],
      "source": [
        "from hsnf import column_style_hermite_normal_form, row_style_hermite_normal_form, smith_normal_form\n",
        "from itertools import product\n",
        "from ticodm.markov_basis import instantiate_markov_basis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ffb5f4e",
      "metadata": {},
      "outputs": [],
      "source": [
        "ct_copy = deepcopy(ct)\n",
        "ct_copy.I = 10\n",
        "ct_copy.J = 10\n",
        "ct_copy.rowsums = list(np.random.multinomial(1000,np.ones(ct_copy.I)*(1/ct_copy.I)))\n",
        "ct_copy.colsums = list(np.random.multinomial(1000,np.ones(ct_copy.J)*(1/ct_copy.J)))\n",
        "ct_copy.table = ct_copy.table_monte_carlo_sample()\n",
        "ct_copy.cells = sorted([tuple(cell) for cell in product(range(ct_copy.I),range(ct_copy.J))])\n",
        "mb = instantiate_markov_basis(ct_copy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2ba6c75",
      "metadata": {},
      "outputs": [],
      "source": [
        "def T(x):\n",
        "    return np.array([1 if (x[0] == r) else 0 for r in range(ct_copy.I)])#.reshape(ct.I,1)\n",
        "def T2(x):\n",
        "    return np.array([1 if (x[0] == c or (x[1]+ct_copy.I) == c) else 0 for c in range(ct_copy.I+ct_copy.J)])#.reshape(ct.I,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5233d269",
      "metadata": {},
      "outputs": [],
      "source": [
        "# A = np.empty((ct_copy.I+ct_copy.J,len(ct_copy)))\n",
        "# for i,c in enumerate(ct_copy.cells):\n",
        "#     A[:,i] = T2(c)\n",
        "    \n",
        "A = np.empty((ct_copy.I,len(ct_copy)))\n",
        "for i,c in enumerate(ct_copy.cells):\n",
        "    A[:,i] = T(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e18b210",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find hermite normal form of A\n",
        "H, U = column_style_hermite_normal_form(A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35265e0a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find basis of kernel of A\n",
        "basis_indices = []\n",
        "basis = []\n",
        "for j in range(H.shape[1]):\n",
        "    if not np.any(H[:,j]):\n",
        "        basis_indices.append(j)\n",
        "for j in basis_indices:\n",
        "    basis.append(U[:,j])\n",
        "#     print(U[:,j].reshape(ct_copy.shape()))\n",
        "#     print('\\n')\n",
        "basis = np.array(basis)\n",
        "\n",
        "print('Markov basis',len(mb))\n",
        "print('Lattice basis',basis.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0827b71",
      "metadata": {},
      "outputs": [],
      "source": [
        "ct_copy.table_admissible(ct_copy.table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ae1b621",
      "metadata": {},
      "outputs": [],
      "source": [
        "ct_copy.table_admissible((ct_copy.table.flatten() + basis[0]).reshape(ct_copy.I,ct_copy.J))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b05ff6a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "ct_copy.table_admissible((ct_copy.table.flatten() + basis[0] + 2*basis[4]).reshape(ct_copy.I,ct_copy.J))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb8bcd04",
      "metadata": {},
      "source": [
        "# Intensity convergence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d718c75",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Expertiment id\n",
        "experiment_id = 'synthetic_2x3_exp9_K200_direct_sampling'\n",
        "# Expertiment type\n",
        "experiment_type = 'TableMCMCConvergence'\n",
        "# Expertiment date\n",
        "date = '26_05_2022'\n",
        "# exp8c_TableMCMC_19_04_2022\n",
        "# Comment\n",
        "comment = 'using_direct_sampling' \n",
        "# comment = 'using_degree_higher_markov_basis'\n",
        "# comment = 'using_degree_one_markov_basis'\n",
        "# comment = 'using_direct_sampling'\n",
        "\n",
        "\n",
        "# Define directory\n",
        "dirpath = f'../data/outputs/{experiment_id}_{experiment_type}_{date}/'\n",
        "# Define filepaths\n",
        "ensemble_metadata_filename = os.path.join(dirpath,f'{experiment_id}_{experiment_type}_{date}_metadata.json')\n",
        "ensemble_table_filename = os.path.join(dirpath,f'samples/table_samples*.npy.gz')\n",
        "ensemble_destination_dem_filename = os.path.join(dirpath,f'samples/destination_demand_samples*.npy.gz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3808eb70",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read samples\n",
        "with open(ensemble_metadata_filename, 'r') as fin:\n",
        "    ensemble_metadata = json.load(fin)\n",
        "ensemble_table_samples = []\n",
        "for file in sorted(glob.glob(ensemble_table_filename)):\n",
        "    # Load files into memory\n",
        "    sam = read_npy(file)\n",
        "    ensemble_table_samples.append(sam)\n",
        "    \n",
        "ensemble_table_samples = np.array(ensemble_table_samples)\n",
        "\n",
        "# Load objects\n",
        "# Reconstruct expected flows \n",
        "ensemble_metadata_copy = deepcopy(ensemble_metadata)\n",
        "ensemble_metadata_copy['mcmc']['contingency_table']['proposal'] = 'direct_sampling'\n",
        "ensemble_dummy_config = Namespace(**{'settings':ensemble_metadata_copy})\n",
        "ct = instantiate_ct(ensemble_dummy_config)\n",
        "\n",
        "# Read important metadata (true latent values)\n",
        "true_table = ct.table\n",
        "colsums = true_table.sum(axis=0)\n",
        "rowsums = true_table.sum(axis=1)\n",
        "I,J = len(rowsums),len(colsums)\n",
        "log_lambdas = np.asarray(np.log(ct.table))\n",
        "log_colsum_lambdas = np.ones(J)\n",
        "for j in range(J):\n",
        "    log_colsum_lambdas[j] = logsumexp(log_lambdas[:,j])\n",
        "N = ensemble_table_samples.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "094ea911",
      "metadata": {},
      "source": [
        "## Convergence of mean intensity estimator\n",
        "This follows [Sheldon's approach](https://proceedings.neurips.cc/paper/2011/file/fccb3cdc9acc14a6e70a12f74560c026-Paper.pdf]) that checks that \n",
        "\n",
        "$\\mathbb{E}\\left[\\mathbb{E}\\left[\\mathbf{n}|\\mathbf{n}_{\\cdot,+}\\right]\\right] \\to \\mathbb{E}\\left[\\mathbf{n}\\right] = \\boldsymbol{\\lambda}$\n",
        "\n",
        "This established weak law of large numbers using the mean estimator\n",
        "$$\\bar{\\mathbf{n}} = \\frac{1}{KM}\\sum_{k=1}^K\\sum_{m=1}^M \\mathbf{n}^{(k,m)}$$\n",
        "\n",
        "where $\\mathbf{n}^{(k,m)} \\sim p(\\mathbf{n}|\\mathbf{n}_{\\cdot,+}^{k})$,$\\;\\; \\mathbf{n}_{\\cdot,+}^{k}\\sim p(\\mathbf{n}_{\\cdot,+})$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9492a85e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define sample sizes so that statistics will be compute every MCMC interval\n",
        "table_sample_step = 1\n",
        "table_burnin = 0\n",
        "table_chain_length = 100\n",
        "maxN = int(min(table_burnin+table_chain_length,ensemble_table_samples.shape[1]))\n",
        "table_sample_sizes = range(table_burnin+table_sample_step,maxN+table_sample_step,table_sample_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b11fe4f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "ensemble_l1_norms = np.zeros(len(table_sample_sizes))\n",
        "ensemble_l2_norms = np.zeros(len(table_sample_sizes))\n",
        "ensemble_mean = np.mean(ensemble_table_samples,axis=0)\n",
        "for i,s in tqdm(enumerate(table_sample_sizes),total=len(table_sample_sizes)):    \n",
        "    # Compute L1 of ensemble mean\n",
        "    ensemble_l1_norms[i] = relative_l1(\n",
        "                                tab=np.mean(ensemble_mean[table_burnin:s,:,:],axis=0).astype('float32'),\n",
        "                                tab0=np.exp(log_lambdas).astype('float32')\n",
        "                        )\n",
        "    # Compute L2 of ensemble mean\n",
        "    ensemble_l2_norms[i] = relative_l2_norm(\n",
        "                                tab=np.mean(ensemble_mean[table_burnin:s,:,:],axis=0).astype('float32'),\n",
        "                                tab0=np.exp(log_lambdas).astype('float32')\n",
        "                        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "501b4aeb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write to file\n",
        "write_npy(\n",
        "    np.array([table_sample_sizes,ensemble_l1_norms]),\n",
        "    os.path.join(dirpath,f'samples/ensemble_k{ensemble_table_samples.shape[0]}_relative_l1_sample_mean_with_mcmc_iteration_{comment}.gz.npy')\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83f0d2fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(table_sample_sizes,ensemble_l1_norms)\n",
        "plt.axhline(y=0,color='red')\n",
        "plt.title(f'Ensemble of {ensemble_table_samples.shape[0]} samplers {comment.replace(\"_\",\" \")}',fontsize=14)\n",
        "plt.xlabel('MCMC iteration',fontsize=16)\n",
        "plt.ylabel(r'Relative $L_1$ norm of $\\mathbb{E}\\left[\\mathbb{E}\\left[\\mathbf{n}|\\mathbf{n}_{\\cdot,+}\\right]\\right]$',fontsize=16)\n",
        "# plt.xticks(sample_sizes[::10])\n",
        "plt.locator_params(axis='x', nbins=20)\n",
        "plt.savefig(os.path.join(dirpath,f'figures/ensemble_k{ensemble_table_samples.shape[0]}_relative_l2_norm_sample_mean_with_mcmc_iteration_chain_length_{table_chain_length}_{comment}.eps'),format='eps')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c24cf6e",
      "metadata": {},
      "source": [
        "## Plot all rates together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "636c2b6d",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "59eed1a3",
      "metadata": {},
      "source": [
        "## Convergence of variance intensity estimator [this is problematic]\n",
        "This checks that\n",
        "\n",
        "$\\mathbb{V}\\left[\\mathbf{n}\\right] = \\boldsymbol{\\lambda} = \\mathbb{E}_{\\mathbf{n}_{\\cdot,+}}\\left[\\mathbb{V}\\left[\\mathbf{n}|\\mathbf{n}_{\\cdot,+} \\right] \\right] + \\mathbb{V}_{\\mathbf{n}_{\\cdot,+}} \\left[ \\mathbb{E}\\left[ \\mathbf{n}|\\mathbf{n}_{\\cdot,+}\\right] \\right]$\n",
        "\n",
        "This established weak law of large numbers using the mean estimators\n",
        "$$\\bar{\\mathbf{n}}= \\frac{1}{KM}\\sum_{k=1}^K\\sum_{m=1}^M \\mathbf{n}^{(k,m)}$$\n",
        "$$\\bar{\\mathbf{n}}^{(k)} = \\frac{1}{M}\\sum_{m=1}^M \\mathbf{n}^{(k,m)}$$\n",
        "$$\\bar{\\mathbf{n}}^{(m)} = \\frac{1}{K}\\sum_{k=1}^K \\mathbf{n}^{(k,m)}$$\n",
        "and the standard deviation estimator of $\\sqrt{\\mathbb{V} \\left[ \\mathbf{n}|\\mathbf{n}_{\\cdot,+}^{(k)} \\right]}$\n",
        "$$\\hat{\\sigma}^{(k,M)}_{\\mathbf{n}|\\mathbf{n_{\\cdot,+}}} = \\sqrt{\\frac{1}{M-1}\\sum_{m=1}^{M} (\\mathbf{n}^{(m,k)} - \\mathbf{\\bar{n}}^{(k)})^2}$$\n",
        "\n",
        "where $\\mathbf{n}^{(k,m)} \\sim p(\\mathbf{n}|\\mathbf{n}_{\\cdot,+}^{k})$,$\\;\\; \\mathbf{n}_{\\cdot,+}^{k}\\sim p(\\mathbf{n}_{\\cdot,+})$. Therefore, we get that \n",
        "\n",
        "$$\\mathbb{V}\\left[\\mathbf{n}\\right] = \\frac{1}{K}\\frac{1}{M-1}\\sum_{k=1}^{K}\\sum_{m=1}^{M} (\\mathbf{n}^{(m,k)} - \\mathbf{\\bar{n}}^{(k)})^2  + \\frac{1}{K-1}\\sum_{k=1}^K\\left(\\mathbf{\\bar{n}}^{(k)} - \\mathbf{\\bar{n}} \\right)^2$$\n",
        "\n",
        "The problem in the above is that we cannot get an unbiased estimator of $\\mathbb{V}\\left[\\mathbf{n}|\\mathbf{n}_{\\cdot,+}\\right]$ easily. For example, using \n",
        "$$\\mathbb{V}\\left[\\mathbf{n}\\right] = \\mathbb{E}\\left[\\mathbf{n}^T\\mathbf{n}|\\mathbf{n}_{\\cdot,+}\\right]- \\left(\\mathbb{E}\\left[\\mathbf{n}|\\mathbf{n}_{\\cdot,+}\\right]\\right)^2$$\n",
        "one cannot easily obtain an unbiased estimator for $\\left(\\mathbb{E}\\left[\\mathbf{n}|\\mathbf{n}_{\\cdot,+}\\right]\\right)^2$  and guarantee that the conditional variance estimator is always non-negative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3043be2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define sample sizes so that statistics will be compute every MCMC interval\n",
        "table_sample_step = 1\n",
        "table_burnin = 0\n",
        "table_chain_length = 1000\n",
        "maxN = int(min(table_burnin+table_chain_length,ensemble_table_samples.shape[1]))\n",
        "table_sample_sizes = range(table_burnin+table_sample_step+1,maxN+table_sample_step+1,table_sample_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "326f9b10",
      "metadata": {},
      "outputs": [],
      "source": [
        "ensemble_variance_l1_norms = np.zeros(len(table_sample_sizes))\n",
        "ensemble_variance_l2_norms = np.zeros(len(table_sample_sizes))\n",
        "ensemble_mean = np.mean(ensemble_table_samples,axis=0)\n",
        "sample_mean = np.mean(ensemble_table_samples,axis=1)\n",
        "for i,s in tqdm(enumerate(table_sample_sizes),total=len(table_sample_sizes)):    \n",
        "    # Compute L1 of ensemble mean\n",
        "    ensemble_variance_l1_norms[i] = relative_l1(\n",
        "                                tab=(\n",
        "                                    np.mean(np.var(ensemble_table_samples[:,table_burnin:s,:,:],axis=1,ddof=1),axis=0) +\\\n",
        "                                    np.var(np.mean(ensemble_table_samples[:,table_burnin:s,:,:],axis=1),axis=0,ddof=1)\n",
        "                                ).astype('float32'),\n",
        "                                tab0=np.exp(log_lambdas).astype('float32')\n",
        "                        )\n",
        "    # Compute L2 of ensemble mean\n",
        "    ensemble_variance_l2_norms[i] = relative_l2_norm(\n",
        "                                tab=(\n",
        "                                    np.mean(np.var(ensemble_table_samples[:,table_burnin:s,:,:],axis=1,ddof=1),axis=0) +\\\n",
        "                                    np.var(np.mean(ensemble_table_samples[:,table_burnin:s,:,:],axis=1),axis=0,ddof=1)\n",
        "                                ).astype('float32'),\n",
        "                                tab0=np.exp(log_lambdas).astype('float32')\n",
        "                        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b1e9ebc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write to file\n",
        "write_npy(\n",
        "    np.array([table_sample_sizes,ensemble_variance_l1_norms]),\n",
        "    os.path.join(dirpath,f'samples/ensemble_k{ensemble_table_samples.shape[0]}_relative_l1_sample_var_with_mcmc_iteration_{comment}.gz.npy')\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51542606",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(table_sample_sizes,ensemble_variance_l1_norms)\n",
        "plt.axhline(y=0,color='red')\n",
        "plt.title(f'Ensemble of {ensemble_table_samples.shape[0]} samplers {comment.replace(\"_\",\" \")}',fontsize=14)\n",
        "plt.xlabel('MCMC iteration',fontsize=16)\n",
        "plt.ylabel(r'Relative $L_1$ norm of $\\mathbb{V}\\left[\\mathbf{n}|\\mathbf{n}_{\\cdot,+}\\right]$',fontsize=16)\n",
        "plt.locator_params(axis='x', nbins=20)\n",
        "plt.savefig(os.path.join(dirpath,f'figures/ensemble_k{ensemble_table_samples.shape[0]}_relative_l2_norm_sample_mean_with_mcmc_iteration_chain_length_{table_chain_length}_{comment}.eps'),format='eps')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03f5daaf",
      "metadata": {},
      "source": [
        "# Convergence of multinomially distributed random variable\n",
        "Script for checking asymptotic distribution of Multinomial for large N"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0c59df3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# size = 3\n",
        "# probs = np.array([0.6,0.3,0.1])\n",
        "# #np.ones(size)*(1/size)\n",
        "# totals = np.linspace(10,1000,5,dtype=int)\n",
        "# histograms = {}\n",
        "# for i,t in enumerate(totals):\n",
        "#     histograms[str(t)] = {}\n",
        "#     print(f'Finding support, i = {i}')\n",
        "#     supp = [x for x in itertools.product(range(1,t), repeat=size) if sum(x) == t]\n",
        "#     normalisation = 0\n",
        "#     for s in tqdm(supp,leave=True):\n",
        "#         histograms[str(t)][table_to_str(s)] = scipy.stats.multinomial.logpmf(x=s,n=t,p=probs).flatten()[0]\n",
        "#         normalisation += np.exp(histograms[str(t)][table_to_str(s)])\n",
        "#     # Renormalise probability\n",
        "#     for s in supp:\n",
        "#         histograms[str(t)][table_to_str(s)] -= np.log(normalisation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4262d150",
      "metadata": {},
      "outputs": [],
      "source": [
        "# fig,axs = plt.subplots(1,len(totals),figsize=(20,10))\n",
        "# for i,t in enumerate(totals):\n",
        "#     axs[i].set_title(f'N = {t}, support size = {len(histograms[str(t)])}')\n",
        "#     axs[i].plot( range(len(histograms[str(t)])), np.exp(list(histograms[str(t)].values()))) \n",
        "# plt.savefig(os.path.join(dirpath,f'figures/asymptotic_multinomial_distribution_large_N_unequal_probs.eps'),format='eps')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ticodm",
      "language": "python",
      "name": "ticodm"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
